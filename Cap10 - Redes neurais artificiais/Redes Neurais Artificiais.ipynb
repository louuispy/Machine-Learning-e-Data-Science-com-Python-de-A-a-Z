{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d364205b",
   "metadata": {},
   "source": [
    "# Machine Learning e Data Science com Python de A Ã  Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86be42",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01529980",
   "metadata": {},
   "source": [
    "## Base credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe5ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65cada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb42841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2760523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b32dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.88520542\n",
      "Iteration 2, loss = 0.85724582\n",
      "Iteration 3, loss = 0.83032203\n",
      "Iteration 4, loss = 0.80534943\n",
      "Iteration 5, loss = 0.78211391\n",
      "Iteration 6, loss = 0.76037165\n",
      "Iteration 7, loss = 0.73989436\n",
      "Iteration 8, loss = 0.72152806\n",
      "Iteration 9, loss = 0.70365713\n",
      "Iteration 10, loss = 0.68778274\n",
      "Iteration 11, loss = 0.67271552\n",
      "Iteration 12, loss = 0.65897155\n",
      "Iteration 13, loss = 0.64611602\n",
      "Iteration 14, loss = 0.63404042\n",
      "Iteration 15, loss = 0.62294342\n",
      "Iteration 16, loss = 0.61238094\n",
      "Iteration 17, loss = 0.60310307\n",
      "Iteration 18, loss = 0.59409279\n",
      "Iteration 19, loss = 0.58580517\n",
      "Iteration 20, loss = 0.57813522\n",
      "Iteration 21, loss = 0.57104935\n",
      "Iteration 22, loss = 0.56420278\n",
      "Iteration 23, loss = 0.55790573\n",
      "Iteration 24, loss = 0.55214113\n",
      "Iteration 25, loss = 0.54646089\n",
      "Iteration 26, loss = 0.54150277\n",
      "Iteration 27, loss = 0.53664370\n",
      "Iteration 28, loss = 0.53211287\n",
      "Iteration 29, loss = 0.52778465\n",
      "Iteration 30, loss = 0.52368432\n",
      "Iteration 31, loss = 0.51987485\n",
      "Iteration 32, loss = 0.51610312\n",
      "Iteration 33, loss = 0.51265584\n",
      "Iteration 34, loss = 0.50930403\n",
      "Iteration 35, loss = 0.50602550\n",
      "Iteration 36, loss = 0.50299039\n",
      "Iteration 37, loss = 0.49996503\n",
      "Iteration 38, loss = 0.49700592\n",
      "Iteration 39, loss = 0.49415560\n",
      "Iteration 40, loss = 0.49133732\n",
      "Iteration 41, loss = 0.48859302\n",
      "Iteration 42, loss = 0.48595522\n",
      "Iteration 43, loss = 0.48324015\n",
      "Iteration 44, loss = 0.48066759\n",
      "Iteration 45, loss = 0.47805357\n",
      "Iteration 46, loss = 0.47549634\n",
      "Iteration 47, loss = 0.47284852\n",
      "Iteration 48, loss = 0.47028417\n",
      "Iteration 49, loss = 0.46766568\n",
      "Iteration 50, loss = 0.46511056\n",
      "Iteration 51, loss = 0.46250095\n",
      "Iteration 52, loss = 0.45994842\n",
      "Iteration 53, loss = 0.45733641\n",
      "Iteration 54, loss = 0.45477562\n",
      "Iteration 55, loss = 0.45221991\n",
      "Iteration 56, loss = 0.44960483\n",
      "Iteration 57, loss = 0.44705003\n",
      "Iteration 58, loss = 0.44439805\n",
      "Iteration 59, loss = 0.44178027\n",
      "Iteration 60, loss = 0.43907666\n",
      "Iteration 61, loss = 0.43643235\n",
      "Iteration 62, loss = 0.43368948\n",
      "Iteration 63, loss = 0.43094092\n",
      "Iteration 64, loss = 0.42811180\n",
      "Iteration 65, loss = 0.42524110\n",
      "Iteration 66, loss = 0.42227404\n",
      "Iteration 67, loss = 0.41930851\n",
      "Iteration 68, loss = 0.41629802\n",
      "Iteration 69, loss = 0.41328830\n",
      "Iteration 70, loss = 0.41020771\n",
      "Iteration 71, loss = 0.40706079\n",
      "Iteration 72, loss = 0.40395874\n",
      "Iteration 73, loss = 0.40077164\n",
      "Iteration 74, loss = 0.39759931\n",
      "Iteration 75, loss = 0.39433790\n",
      "Iteration 76, loss = 0.39108156\n",
      "Iteration 77, loss = 0.38781872\n",
      "Iteration 78, loss = 0.38446512\n",
      "Iteration 79, loss = 0.38116386\n",
      "Iteration 80, loss = 0.37782386\n",
      "Iteration 81, loss = 0.37450400\n",
      "Iteration 82, loss = 0.37110582\n",
      "Iteration 83, loss = 0.36779787\n",
      "Iteration 84, loss = 0.36447040\n",
      "Iteration 85, loss = 0.36115790\n",
      "Iteration 86, loss = 0.35776365\n",
      "Iteration 87, loss = 0.35440485\n",
      "Iteration 88, loss = 0.35096960\n",
      "Iteration 89, loss = 0.34760852\n",
      "Iteration 90, loss = 0.34418752\n",
      "Iteration 91, loss = 0.34075583\n",
      "Iteration 92, loss = 0.33737752\n",
      "Iteration 93, loss = 0.33403522\n",
      "Iteration 94, loss = 0.33066619\n",
      "Iteration 95, loss = 0.32737798\n",
      "Iteration 96, loss = 0.32408608\n",
      "Iteration 97, loss = 0.32091993\n",
      "Iteration 98, loss = 0.31791407\n",
      "Iteration 99, loss = 0.31500800\n",
      "Iteration 100, loss = 0.31221881\n",
      "Iteration 101, loss = 0.30952983\n",
      "Iteration 102, loss = 0.30689401\n",
      "Iteration 103, loss = 0.30428931\n",
      "Iteration 104, loss = 0.30178061\n",
      "Iteration 105, loss = 0.29939355\n",
      "Iteration 106, loss = 0.29707122\n",
      "Iteration 107, loss = 0.29481850\n",
      "Iteration 108, loss = 0.29267237\n",
      "Iteration 109, loss = 0.29051982\n",
      "Iteration 110, loss = 0.28848703\n",
      "Iteration 111, loss = 0.28659267\n",
      "Iteration 112, loss = 0.28466578\n",
      "Iteration 113, loss = 0.28286967\n",
      "Iteration 114, loss = 0.28112491\n",
      "Iteration 115, loss = 0.27947064\n",
      "Iteration 116, loss = 0.27790285\n",
      "Iteration 117, loss = 0.27647780\n",
      "Iteration 118, loss = 0.27504532\n",
      "Iteration 119, loss = 0.27369028\n",
      "Iteration 120, loss = 0.27236397\n",
      "Iteration 121, loss = 0.27107742\n",
      "Iteration 122, loss = 0.26981923\n",
      "Iteration 123, loss = 0.26855908\n",
      "Iteration 124, loss = 0.26735339\n",
      "Iteration 125, loss = 0.26617095\n",
      "Iteration 126, loss = 0.26501946\n",
      "Iteration 127, loss = 0.26388141\n",
      "Iteration 128, loss = 0.26276949\n",
      "Iteration 129, loss = 0.26170406\n",
      "Iteration 130, loss = 0.26064587\n",
      "Iteration 131, loss = 0.25961401\n",
      "Iteration 132, loss = 0.25860208\n",
      "Iteration 133, loss = 0.25757087\n",
      "Iteration 134, loss = 0.25657839\n",
      "Iteration 135, loss = 0.25558302\n",
      "Iteration 136, loss = 0.25459587\n",
      "Iteration 137, loss = 0.25361974\n",
      "Iteration 138, loss = 0.25266123\n",
      "Iteration 139, loss = 0.25171778\n",
      "Iteration 140, loss = 0.25080091\n",
      "Iteration 141, loss = 0.24986785\n",
      "Iteration 142, loss = 0.24896160\n",
      "Iteration 143, loss = 0.24805240\n",
      "Iteration 144, loss = 0.24718104\n",
      "Iteration 145, loss = 0.24631036\n",
      "Iteration 146, loss = 0.24539224\n",
      "Iteration 147, loss = 0.24452432\n",
      "Iteration 148, loss = 0.24365527\n",
      "Iteration 149, loss = 0.24280942\n",
      "Iteration 150, loss = 0.24198361\n",
      "Iteration 151, loss = 0.24116908\n",
      "Iteration 152, loss = 0.24035101\n",
      "Iteration 153, loss = 0.23954427\n",
      "Iteration 154, loss = 0.23877157\n",
      "Iteration 155, loss = 0.23796611\n",
      "Iteration 156, loss = 0.23717795\n",
      "Iteration 157, loss = 0.23640115\n",
      "Iteration 158, loss = 0.23562935\n",
      "Iteration 159, loss = 0.23484770\n",
      "Iteration 160, loss = 0.23410559\n",
      "Iteration 161, loss = 0.23336384\n",
      "Iteration 162, loss = 0.23261312\n",
      "Iteration 163, loss = 0.23188794\n",
      "Iteration 164, loss = 0.23116124\n",
      "Iteration 165, loss = 0.23041795\n",
      "Iteration 166, loss = 0.22970101\n",
      "Iteration 167, loss = 0.22899932\n",
      "Iteration 168, loss = 0.22826001\n",
      "Iteration 169, loss = 0.22755329\n",
      "Iteration 170, loss = 0.22683799\n",
      "Iteration 171, loss = 0.22614208\n",
      "Iteration 172, loss = 0.22544445\n",
      "Iteration 173, loss = 0.22473846\n",
      "Iteration 174, loss = 0.22404753\n",
      "Iteration 175, loss = 0.22334946\n",
      "Iteration 176, loss = 0.22266102\n",
      "Iteration 177, loss = 0.22200361\n",
      "Iteration 178, loss = 0.22130299\n",
      "Iteration 179, loss = 0.22063933\n",
      "Iteration 180, loss = 0.21999156\n",
      "Iteration 181, loss = 0.21933361\n",
      "Iteration 182, loss = 0.21867177\n",
      "Iteration 183, loss = 0.21803776\n",
      "Iteration 184, loss = 0.21739601\n",
      "Iteration 185, loss = 0.21675925\n",
      "Iteration 186, loss = 0.21611577\n",
      "Iteration 187, loss = 0.21547061\n",
      "Iteration 188, loss = 0.21484752\n",
      "Iteration 189, loss = 0.21421903\n",
      "Iteration 190, loss = 0.21359039\n",
      "Iteration 191, loss = 0.21297398\n",
      "Iteration 192, loss = 0.21235749\n",
      "Iteration 193, loss = 0.21173311\n",
      "Iteration 194, loss = 0.21114727\n",
      "Iteration 195, loss = 0.21052424\n",
      "Iteration 196, loss = 0.20993449\n",
      "Iteration 197, loss = 0.20931443\n",
      "Iteration 198, loss = 0.20871837\n",
      "Iteration 199, loss = 0.20816210\n",
      "Iteration 200, loss = 0.20754337\n",
      "Iteration 201, loss = 0.20695770\n",
      "Iteration 202, loss = 0.20636372\n",
      "Iteration 203, loss = 0.20577275\n",
      "Iteration 204, loss = 0.20519151\n",
      "Iteration 205, loss = 0.20461154\n",
      "Iteration 206, loss = 0.20405124\n",
      "Iteration 207, loss = 0.20344930\n",
      "Iteration 208, loss = 0.20287456\n",
      "Iteration 209, loss = 0.20230982\n",
      "Iteration 210, loss = 0.20175477\n",
      "Iteration 211, loss = 0.20117367\n",
      "Iteration 212, loss = 0.20061444\n",
      "Iteration 213, loss = 0.20006803\n",
      "Iteration 214, loss = 0.19950566\n",
      "Iteration 215, loss = 0.19894985\n",
      "Iteration 216, loss = 0.19842143\n",
      "Iteration 217, loss = 0.19785523\n",
      "Iteration 218, loss = 0.19730450\n",
      "Iteration 219, loss = 0.19676589\n",
      "Iteration 220, loss = 0.19623123\n",
      "Iteration 221, loss = 0.19569081\n",
      "Iteration 222, loss = 0.19516691\n",
      "Iteration 223, loss = 0.19463746\n",
      "Iteration 224, loss = 0.19411453\n",
      "Iteration 225, loss = 0.19359614\n",
      "Iteration 226, loss = 0.19309816\n",
      "Iteration 227, loss = 0.19256278\n",
      "Iteration 228, loss = 0.19205168\n",
      "Iteration 229, loss = 0.19153768\n",
      "Iteration 230, loss = 0.19102453\n",
      "Iteration 231, loss = 0.19051978\n",
      "Iteration 232, loss = 0.19002705\n",
      "Iteration 233, loss = 0.18951039\n",
      "Iteration 234, loss = 0.18901236\n",
      "Iteration 235, loss = 0.18851326\n",
      "Iteration 236, loss = 0.18801385\n",
      "Iteration 237, loss = 0.18751503\n",
      "Iteration 238, loss = 0.18702266\n",
      "Iteration 239, loss = 0.18652816\n",
      "Iteration 240, loss = 0.18602365\n",
      "Iteration 241, loss = 0.18555164\n",
      "Iteration 242, loss = 0.18503893\n",
      "Iteration 243, loss = 0.18454698\n",
      "Iteration 244, loss = 0.18406952\n",
      "Iteration 245, loss = 0.18357114\n",
      "Iteration 246, loss = 0.18309603\n",
      "Iteration 247, loss = 0.18258733\n",
      "Iteration 248, loss = 0.18211367\n",
      "Iteration 249, loss = 0.18162581\n",
      "Iteration 250, loss = 0.18115484\n",
      "Iteration 251, loss = 0.18065052\n",
      "Iteration 252, loss = 0.18018398\n",
      "Iteration 253, loss = 0.17970150\n",
      "Iteration 254, loss = 0.17918674\n",
      "Iteration 255, loss = 0.17872249\n",
      "Iteration 256, loss = 0.17823281\n",
      "Iteration 257, loss = 0.17775317\n",
      "Iteration 258, loss = 0.17726416\n",
      "Iteration 259, loss = 0.17677162\n",
      "Iteration 260, loss = 0.17629375\n",
      "Iteration 261, loss = 0.17579836\n",
      "Iteration 262, loss = 0.17532226\n",
      "Iteration 263, loss = 0.17484254\n",
      "Iteration 264, loss = 0.17435511\n",
      "Iteration 265, loss = 0.17386693\n",
      "Iteration 266, loss = 0.17338659\n",
      "Iteration 267, loss = 0.17291075\n",
      "Iteration 268, loss = 0.17243327\n",
      "Iteration 269, loss = 0.17196187\n",
      "Iteration 270, loss = 0.17148535\n",
      "Iteration 271, loss = 0.17101490\n",
      "Iteration 272, loss = 0.17053658\n",
      "Iteration 273, loss = 0.17007243\n",
      "Iteration 274, loss = 0.16958906\n",
      "Iteration 275, loss = 0.16910862\n",
      "Iteration 276, loss = 0.16863162\n",
      "Iteration 277, loss = 0.16818011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 278, loss = 0.16769605\n",
      "Iteration 279, loss = 0.16721528\n",
      "Iteration 280, loss = 0.16673020\n",
      "Iteration 281, loss = 0.16627546\n",
      "Iteration 282, loss = 0.16579093\n",
      "Iteration 283, loss = 0.16531514\n",
      "Iteration 284, loss = 0.16486048\n",
      "Iteration 285, loss = 0.16437713\n",
      "Iteration 286, loss = 0.16392298\n",
      "Iteration 287, loss = 0.16344255\n",
      "Iteration 288, loss = 0.16297583\n",
      "Iteration 289, loss = 0.16251091\n",
      "Iteration 290, loss = 0.16203543\n",
      "Iteration 291, loss = 0.16156932\n",
      "Iteration 292, loss = 0.16111501\n",
      "Iteration 293, loss = 0.16065563\n",
      "Iteration 294, loss = 0.16019553\n",
      "Iteration 295, loss = 0.15973512\n",
      "Iteration 296, loss = 0.15925540\n",
      "Iteration 297, loss = 0.15879063\n",
      "Iteration 298, loss = 0.15833971\n",
      "Iteration 299, loss = 0.15788855\n",
      "Iteration 300, loss = 0.15741899\n",
      "Iteration 301, loss = 0.15695742\n",
      "Iteration 302, loss = 0.15651852\n",
      "Iteration 303, loss = 0.15604729\n",
      "Iteration 304, loss = 0.15555910\n",
      "Iteration 305, loss = 0.15510626\n",
      "Iteration 306, loss = 0.15464670\n",
      "Iteration 307, loss = 0.15419129\n",
      "Iteration 308, loss = 0.15372447\n",
      "Iteration 309, loss = 0.15326899\n",
      "Iteration 310, loss = 0.15280854\n",
      "Iteration 311, loss = 0.15236104\n",
      "Iteration 312, loss = 0.15189838\n",
      "Iteration 313, loss = 0.15143418\n",
      "Iteration 314, loss = 0.15099174\n",
      "Iteration 315, loss = 0.15054540\n",
      "Iteration 316, loss = 0.15008436\n",
      "Iteration 317, loss = 0.14960749\n",
      "Iteration 318, loss = 0.14915372\n",
      "Iteration 319, loss = 0.14869466\n",
      "Iteration 320, loss = 0.14824756\n",
      "Iteration 321, loss = 0.14776629\n",
      "Iteration 322, loss = 0.14730054\n",
      "Iteration 323, loss = 0.14684048\n",
      "Iteration 324, loss = 0.14640015\n",
      "Iteration 325, loss = 0.14592183\n",
      "Iteration 326, loss = 0.14545457\n",
      "Iteration 327, loss = 0.14500981\n",
      "Iteration 328, loss = 0.14453937\n",
      "Iteration 329, loss = 0.14405109\n",
      "Iteration 330, loss = 0.14358011\n",
      "Iteration 331, loss = 0.14312701\n",
      "Iteration 332, loss = 0.14265637\n",
      "Iteration 333, loss = 0.14218481\n",
      "Iteration 334, loss = 0.14171155\n",
      "Iteration 335, loss = 0.14124900\n",
      "Iteration 336, loss = 0.14077866\n",
      "Iteration 337, loss = 0.14035082\n",
      "Iteration 338, loss = 0.13987379\n",
      "Iteration 339, loss = 0.13944048\n",
      "Iteration 340, loss = 0.13893523\n",
      "Iteration 341, loss = 0.13850033\n",
      "Iteration 342, loss = 0.13801262\n",
      "Iteration 343, loss = 0.13755156\n",
      "Iteration 344, loss = 0.13706151\n",
      "Iteration 345, loss = 0.13660617\n",
      "Iteration 346, loss = 0.13613054\n",
      "Iteration 347, loss = 0.13567094\n",
      "Iteration 348, loss = 0.13518491\n",
      "Iteration 349, loss = 0.13472068\n",
      "Iteration 350, loss = 0.13424927\n",
      "Iteration 351, loss = 0.13377979\n",
      "Iteration 352, loss = 0.13333085\n",
      "Iteration 353, loss = 0.13288803\n",
      "Iteration 354, loss = 0.13242674\n",
      "Iteration 355, loss = 0.13197662\n",
      "Iteration 356, loss = 0.13151667\n",
      "Iteration 357, loss = 0.13108806\n",
      "Iteration 358, loss = 0.13062608\n",
      "Iteration 359, loss = 0.13018642\n",
      "Iteration 360, loss = 0.12973297\n",
      "Iteration 361, loss = 0.12928585\n",
      "Iteration 362, loss = 0.12883716\n",
      "Iteration 363, loss = 0.12839669\n",
      "Iteration 364, loss = 0.12793509\n",
      "Iteration 365, loss = 0.12750365\n",
      "Iteration 366, loss = 0.12707751\n",
      "Iteration 367, loss = 0.12661434\n",
      "Iteration 368, loss = 0.12619339\n",
      "Iteration 369, loss = 0.12575751\n",
      "Iteration 370, loss = 0.12531626\n",
      "Iteration 371, loss = 0.12488580\n",
      "Iteration 372, loss = 0.12443932\n",
      "Iteration 373, loss = 0.12402674\n",
      "Iteration 374, loss = 0.12359583\n",
      "Iteration 375, loss = 0.12316436\n",
      "Iteration 376, loss = 0.12273165\n",
      "Iteration 377, loss = 0.12230455\n",
      "Iteration 378, loss = 0.12187885\n",
      "Iteration 379, loss = 0.12147072\n",
      "Iteration 380, loss = 0.12105076\n",
      "Iteration 381, loss = 0.12063027\n",
      "Iteration 382, loss = 0.12020036\n",
      "Iteration 383, loss = 0.11977123\n",
      "Iteration 384, loss = 0.11935136\n",
      "Iteration 385, loss = 0.11896018\n",
      "Iteration 386, loss = 0.11853494\n",
      "Iteration 387, loss = 0.11812319\n",
      "Iteration 388, loss = 0.11768744\n",
      "Iteration 389, loss = 0.11729433\n",
      "Iteration 390, loss = 0.11688009\n",
      "Iteration 391, loss = 0.11647059\n",
      "Iteration 392, loss = 0.11608506\n",
      "Iteration 393, loss = 0.11566314\n",
      "Iteration 394, loss = 0.11525618\n",
      "Iteration 395, loss = 0.11486099\n",
      "Iteration 396, loss = 0.11447798\n",
      "Iteration 397, loss = 0.11404410\n",
      "Iteration 398, loss = 0.11364591\n",
      "Iteration 399, loss = 0.11327208\n",
      "Iteration 400, loss = 0.11288152\n",
      "Iteration 401, loss = 0.11247649\n",
      "Iteration 402, loss = 0.11208886\n",
      "Iteration 403, loss = 0.11168682\n",
      "Iteration 404, loss = 0.11134701\n",
      "Iteration 405, loss = 0.11091047\n",
      "Iteration 406, loss = 0.11054011\n",
      "Iteration 407, loss = 0.11015565\n",
      "Iteration 408, loss = 0.10976152\n",
      "Iteration 409, loss = 0.10940120\n",
      "Iteration 410, loss = 0.10903486\n",
      "Iteration 411, loss = 0.10866218\n",
      "Iteration 412, loss = 0.10828540\n",
      "Iteration 413, loss = 0.10791418\n",
      "Iteration 414, loss = 0.10754698\n",
      "Iteration 415, loss = 0.10719958\n",
      "Iteration 416, loss = 0.10678748\n",
      "Iteration 417, loss = 0.10642045\n",
      "Iteration 418, loss = 0.10607087\n",
      "Iteration 419, loss = 0.10569541\n",
      "Iteration 420, loss = 0.10530886\n",
      "Iteration 421, loss = 0.10495574\n",
      "Iteration 422, loss = 0.10459590\n",
      "Iteration 423, loss = 0.10426208\n",
      "Iteration 424, loss = 0.10389418\n",
      "Iteration 425, loss = 0.10353341\n",
      "Iteration 426, loss = 0.10318494\n",
      "Iteration 427, loss = 0.10285160\n",
      "Iteration 428, loss = 0.10248337\n",
      "Iteration 429, loss = 0.10211532\n",
      "Iteration 430, loss = 0.10178542\n",
      "Iteration 431, loss = 0.10142793\n",
      "Iteration 432, loss = 0.10107842\n",
      "Iteration 433, loss = 0.10071308\n",
      "Iteration 434, loss = 0.10036005\n",
      "Iteration 435, loss = 0.09998798\n",
      "Iteration 436, loss = 0.09965372\n",
      "Iteration 437, loss = 0.09928107\n",
      "Iteration 438, loss = 0.09892012\n",
      "Iteration 439, loss = 0.09859053\n",
      "Iteration 440, loss = 0.09824036\n",
      "Iteration 441, loss = 0.09790101\n",
      "Iteration 442, loss = 0.09755276\n",
      "Iteration 443, loss = 0.09721207\n",
      "Iteration 444, loss = 0.09690122\n",
      "Iteration 445, loss = 0.09654737\n",
      "Iteration 446, loss = 0.09623957\n",
      "Iteration 447, loss = 0.09589364\n",
      "Iteration 448, loss = 0.09556001\n",
      "Iteration 449, loss = 0.09523248\n",
      "Iteration 450, loss = 0.09490905\n",
      "Iteration 451, loss = 0.09456981\n",
      "Iteration 452, loss = 0.09425483\n",
      "Iteration 453, loss = 0.09393173\n",
      "Iteration 454, loss = 0.09359829\n",
      "Iteration 455, loss = 0.09325573\n",
      "Iteration 456, loss = 0.09294156\n",
      "Iteration 457, loss = 0.09260884\n",
      "Iteration 458, loss = 0.09228913\n",
      "Iteration 459, loss = 0.09194869\n",
      "Iteration 460, loss = 0.09162469\n",
      "Iteration 461, loss = 0.09129854\n",
      "Iteration 462, loss = 0.09099614\n",
      "Iteration 463, loss = 0.09065442\n",
      "Iteration 464, loss = 0.09034404\n",
      "Iteration 465, loss = 0.09005726\n",
      "Iteration 466, loss = 0.08973847\n",
      "Iteration 467, loss = 0.08942313\n",
      "Iteration 468, loss = 0.08910290\n",
      "Iteration 469, loss = 0.08876938\n",
      "Iteration 470, loss = 0.08845387\n",
      "Iteration 471, loss = 0.08815272\n",
      "Iteration 472, loss = 0.08783229\n",
      "Iteration 473, loss = 0.08753721\n",
      "Iteration 474, loss = 0.08723528\n",
      "Iteration 475, loss = 0.08689600\n",
      "Iteration 476, loss = 0.08658360\n",
      "Iteration 477, loss = 0.08628388\n",
      "Iteration 478, loss = 0.08599327\n",
      "Iteration 479, loss = 0.08568598\n",
      "Iteration 480, loss = 0.08534136\n",
      "Iteration 481, loss = 0.08503056\n",
      "Iteration 482, loss = 0.08473906\n",
      "Iteration 483, loss = 0.08441819\n",
      "Iteration 484, loss = 0.08410749\n",
      "Iteration 485, loss = 0.08381038\n",
      "Iteration 486, loss = 0.08350533\n",
      "Iteration 487, loss = 0.08318282\n",
      "Iteration 488, loss = 0.08287271\n",
      "Iteration 489, loss = 0.08258738\n",
      "Iteration 490, loss = 0.08230672\n",
      "Iteration 491, loss = 0.08199748\n",
      "Iteration 492, loss = 0.08170491\n",
      "Iteration 493, loss = 0.08140162\n",
      "Iteration 494, loss = 0.08111277\n",
      "Iteration 495, loss = 0.08083077\n",
      "Iteration 496, loss = 0.08052597\n",
      "Iteration 497, loss = 0.08024832\n",
      "Iteration 498, loss = 0.07997403\n",
      "Iteration 499, loss = 0.07968261\n",
      "Iteration 500, loss = 0.07938595\n",
      "Iteration 501, loss = 0.07911935\n",
      "Iteration 502, loss = 0.07881806\n",
      "Iteration 503, loss = 0.07854514\n",
      "Iteration 504, loss = 0.07826818\n",
      "Iteration 505, loss = 0.07798745\n",
      "Iteration 506, loss = 0.07769647\n",
      "Iteration 507, loss = 0.07742623\n",
      "Iteration 508, loss = 0.07713575\n",
      "Iteration 509, loss = 0.07689156\n",
      "Iteration 510, loss = 0.07659997\n",
      "Iteration 511, loss = 0.07632876\n",
      "Iteration 512, loss = 0.07606217\n",
      "Iteration 513, loss = 0.07579539\n",
      "Iteration 514, loss = 0.07551837\n",
      "Iteration 515, loss = 0.07524419\n",
      "Iteration 516, loss = 0.07497289\n",
      "Iteration 517, loss = 0.07470563\n",
      "Iteration 518, loss = 0.07443738\n",
      "Iteration 519, loss = 0.07421247\n",
      "Iteration 520, loss = 0.07390796\n",
      "Iteration 521, loss = 0.07364008\n",
      "Iteration 522, loss = 0.07341301\n",
      "Iteration 523, loss = 0.07312869\n",
      "Iteration 524, loss = 0.07287937\n",
      "Iteration 525, loss = 0.07260349\n",
      "Iteration 526, loss = 0.07236313\n",
      "Iteration 527, loss = 0.07209374\n",
      "Iteration 528, loss = 0.07185851\n",
      "Iteration 529, loss = 0.07158689\n",
      "Iteration 530, loss = 0.07136361\n",
      "Iteration 531, loss = 0.07110740\n",
      "Iteration 532, loss = 0.07083291\n",
      "Iteration 533, loss = 0.07058173\n",
      "Iteration 534, loss = 0.07033557\n",
      "Iteration 535, loss = 0.07009946\n",
      "Iteration 536, loss = 0.06984552\n",
      "Iteration 537, loss = 0.06958716\n",
      "Iteration 538, loss = 0.06935438\n",
      "Iteration 539, loss = 0.06911807\n",
      "Iteration 540, loss = 0.06887883\n",
      "Iteration 541, loss = 0.06862597\n",
      "Iteration 542, loss = 0.06838548\n",
      "Iteration 543, loss = 0.06816113\n",
      "Iteration 544, loss = 0.06792350\n",
      "Iteration 545, loss = 0.06764789\n",
      "Iteration 546, loss = 0.06743724\n",
      "Iteration 547, loss = 0.06716773\n",
      "Iteration 548, loss = 0.06693433\n",
      "Iteration 549, loss = 0.06668995\n",
      "Iteration 550, loss = 0.06646020\n",
      "Iteration 551, loss = 0.06620805\n",
      "Iteration 552, loss = 0.06596126\n",
      "Iteration 553, loss = 0.06573318\n",
      "Iteration 554, loss = 0.06549953\n",
      "Iteration 555, loss = 0.06525250\n",
      "Iteration 556, loss = 0.06502165\n",
      "Iteration 557, loss = 0.06478872\n",
      "Iteration 558, loss = 0.06457177\n",
      "Iteration 559, loss = 0.06433248\n",
      "Iteration 560, loss = 0.06410668\n",
      "Iteration 561, loss = 0.06388696\n",
      "Iteration 562, loss = 0.06365119\n",
      "Iteration 563, loss = 0.06343361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 564, loss = 0.06320424\n",
      "Iteration 565, loss = 0.06297389\n",
      "Iteration 566, loss = 0.06274887\n",
      "Iteration 567, loss = 0.06255282\n",
      "Iteration 568, loss = 0.06228764\n",
      "Iteration 569, loss = 0.06208035\n",
      "Iteration 570, loss = 0.06187694\n",
      "Iteration 571, loss = 0.06163405\n",
      "Iteration 572, loss = 0.06142166\n",
      "Iteration 573, loss = 0.06120759\n",
      "Iteration 574, loss = 0.06100000\n",
      "Iteration 575, loss = 0.06077878\n",
      "Iteration 576, loss = 0.06056191\n",
      "Iteration 577, loss = 0.06034606\n",
      "Iteration 578, loss = 0.06013309\n",
      "Iteration 579, loss = 0.05999427\n",
      "Iteration 580, loss = 0.05973891\n",
      "Iteration 581, loss = 0.05950959\n",
      "Iteration 582, loss = 0.05930312\n",
      "Iteration 583, loss = 0.05911189\n",
      "Iteration 584, loss = 0.05889643\n",
      "Iteration 585, loss = 0.05867415\n",
      "Iteration 586, loss = 0.05848382\n",
      "Iteration 587, loss = 0.05825790\n",
      "Iteration 588, loss = 0.05805327\n",
      "Iteration 589, loss = 0.05785932\n",
      "Iteration 590, loss = 0.05764052\n",
      "Iteration 591, loss = 0.05743175\n",
      "Iteration 592, loss = 0.05723222\n",
      "Iteration 593, loss = 0.05701719\n",
      "Iteration 594, loss = 0.05681681\n",
      "Iteration 595, loss = 0.05661955\n",
      "Iteration 596, loss = 0.05643002\n",
      "Iteration 597, loss = 0.05623020\n",
      "Iteration 598, loss = 0.05602682\n",
      "Iteration 599, loss = 0.05583960\n",
      "Iteration 600, loss = 0.05564137\n",
      "Iteration 601, loss = 0.05543906\n",
      "Iteration 602, loss = 0.05524938\n",
      "Iteration 603, loss = 0.05506421\n",
      "Iteration 604, loss = 0.05485966\n",
      "Iteration 605, loss = 0.05466513\n",
      "Iteration 606, loss = 0.05448571\n",
      "Iteration 607, loss = 0.05428770\n",
      "Iteration 608, loss = 0.05408427\n",
      "Iteration 609, loss = 0.05390714\n",
      "Iteration 610, loss = 0.05373790\n",
      "Iteration 611, loss = 0.05354290\n",
      "Iteration 612, loss = 0.05334905\n",
      "Iteration 613, loss = 0.05317690\n",
      "Iteration 614, loss = 0.05298690\n",
      "Iteration 615, loss = 0.05281727\n",
      "Iteration 616, loss = 0.05262939\n",
      "Iteration 617, loss = 0.05244201\n",
      "Iteration 618, loss = 0.05225683\n",
      "Iteration 619, loss = 0.05207898\n",
      "Iteration 620, loss = 0.05190646\n",
      "Iteration 621, loss = 0.05172861\n",
      "Iteration 622, loss = 0.05156143\n",
      "Iteration 623, loss = 0.05140377\n",
      "Iteration 624, loss = 0.05121395\n",
      "Iteration 625, loss = 0.05104770\n",
      "Iteration 626, loss = 0.05087943\n",
      "Iteration 627, loss = 0.05070764\n",
      "Iteration 628, loss = 0.05052008\n",
      "Iteration 629, loss = 0.05034143\n",
      "Iteration 630, loss = 0.05017213\n",
      "Iteration 631, loss = 0.05002368\n",
      "Iteration 632, loss = 0.04983036\n",
      "Iteration 633, loss = 0.04966264\n",
      "Iteration 634, loss = 0.04954606\n",
      "Iteration 635, loss = 0.04932365\n",
      "Iteration 636, loss = 0.04915407\n",
      "Iteration 637, loss = 0.04900008\n",
      "Iteration 638, loss = 0.04884486\n",
      "Iteration 639, loss = 0.04868601\n",
      "Iteration 640, loss = 0.04849832\n",
      "Iteration 641, loss = 0.04835669\n",
      "Iteration 642, loss = 0.04820437\n",
      "Iteration 643, loss = 0.04802293\n",
      "Iteration 644, loss = 0.04785459\n",
      "Iteration 645, loss = 0.04771536\n",
      "Iteration 646, loss = 0.04757028\n",
      "Iteration 647, loss = 0.04738843\n",
      "Iteration 648, loss = 0.04724892\n",
      "Iteration 649, loss = 0.04707394\n",
      "Iteration 650, loss = 0.04694322\n",
      "Iteration 651, loss = 0.04677738\n",
      "Iteration 652, loss = 0.04663980\n",
      "Iteration 653, loss = 0.04646249\n",
      "Iteration 654, loss = 0.04630940\n",
      "Iteration 655, loss = 0.04616134\n",
      "Iteration 656, loss = 0.04600480\n",
      "Iteration 657, loss = 0.04584662\n",
      "Iteration 658, loss = 0.04569684\n",
      "Iteration 659, loss = 0.04553685\n",
      "Iteration 660, loss = 0.04538513\n",
      "Iteration 661, loss = 0.04526072\n",
      "Iteration 662, loss = 0.04511421\n",
      "Iteration 663, loss = 0.04494985\n",
      "Iteration 664, loss = 0.04481234\n",
      "Iteration 665, loss = 0.04465974\n",
      "Iteration 666, loss = 0.04452741\n",
      "Iteration 667, loss = 0.04435455\n",
      "Iteration 668, loss = 0.04422711\n",
      "Iteration 669, loss = 0.04408777\n",
      "Iteration 670, loss = 0.04392706\n",
      "Iteration 671, loss = 0.04379228\n",
      "Iteration 672, loss = 0.04363450\n",
      "Iteration 673, loss = 0.04350949\n",
      "Iteration 674, loss = 0.04336883\n",
      "Iteration 675, loss = 0.04322283\n",
      "Iteration 676, loss = 0.04308724\n",
      "Iteration 677, loss = 0.04294297\n",
      "Iteration 678, loss = 0.04279938\n",
      "Iteration 679, loss = 0.04264574\n",
      "Iteration 680, loss = 0.04250569\n",
      "Iteration 681, loss = 0.04236517\n",
      "Iteration 682, loss = 0.04222153\n",
      "Iteration 683, loss = 0.04210802\n",
      "Iteration 684, loss = 0.04192812\n",
      "Iteration 685, loss = 0.04178504\n",
      "Iteration 686, loss = 0.04164416\n",
      "Iteration 687, loss = 0.04152177\n",
      "Iteration 688, loss = 0.04138855\n",
      "Iteration 689, loss = 0.04123548\n",
      "Iteration 690, loss = 0.04109791\n",
      "Iteration 691, loss = 0.04096237\n",
      "Iteration 692, loss = 0.04090971\n",
      "Iteration 693, loss = 0.04068887\n",
      "Iteration 694, loss = 0.04054234\n",
      "Iteration 695, loss = 0.04041608\n",
      "Iteration 696, loss = 0.04026787\n",
      "Iteration 697, loss = 0.04014600\n",
      "Iteration 698, loss = 0.04001835\n",
      "Iteration 699, loss = 0.03988108\n",
      "Iteration 700, loss = 0.03973050\n",
      "Iteration 701, loss = 0.03963349\n",
      "Iteration 702, loss = 0.03946851\n",
      "Iteration 703, loss = 0.03934819\n",
      "Iteration 704, loss = 0.03923954\n",
      "Iteration 705, loss = 0.03908312\n",
      "Iteration 706, loss = 0.03897996\n",
      "Iteration 707, loss = 0.03883531\n",
      "Iteration 708, loss = 0.03869164\n",
      "Iteration 709, loss = 0.03856295\n",
      "Iteration 710, loss = 0.03845436\n",
      "Iteration 711, loss = 0.03832442\n",
      "Iteration 712, loss = 0.03820323\n",
      "Iteration 713, loss = 0.03808443\n",
      "Iteration 714, loss = 0.03794915\n",
      "Iteration 715, loss = 0.03779892\n",
      "Iteration 716, loss = 0.03769333\n",
      "Iteration 717, loss = 0.03757510\n",
      "Iteration 718, loss = 0.03745277\n",
      "Iteration 719, loss = 0.03736299\n",
      "Iteration 720, loss = 0.03721025\n",
      "Iteration 721, loss = 0.03708749\n",
      "Iteration 722, loss = 0.03696210\n",
      "Iteration 723, loss = 0.03686735\n",
      "Iteration 724, loss = 0.03672278\n",
      "Iteration 725, loss = 0.03661041\n",
      "Iteration 726, loss = 0.03651945\n",
      "Iteration 727, loss = 0.03636906\n",
      "Iteration 728, loss = 0.03625486\n",
      "Iteration 729, loss = 0.03614886\n",
      "Iteration 730, loss = 0.03602096\n",
      "Iteration 731, loss = 0.03590797\n",
      "Iteration 732, loss = 0.03578362\n",
      "Iteration 733, loss = 0.03569314\n",
      "Iteration 734, loss = 0.03559326\n",
      "Iteration 735, loss = 0.03546070\n",
      "Iteration 736, loss = 0.03533581\n",
      "Iteration 737, loss = 0.03522476\n",
      "Iteration 738, loss = 0.03508909\n",
      "Iteration 739, loss = 0.03499135\n",
      "Iteration 740, loss = 0.03487433\n",
      "Iteration 741, loss = 0.03476063\n",
      "Iteration 742, loss = 0.03465914\n",
      "Iteration 743, loss = 0.03455191\n",
      "Iteration 744, loss = 0.03442638\n",
      "Iteration 745, loss = 0.03432743\n",
      "Iteration 746, loss = 0.03421745\n",
      "Iteration 747, loss = 0.03409256\n",
      "Iteration 748, loss = 0.03402510\n",
      "Iteration 749, loss = 0.03387998\n",
      "Iteration 750, loss = 0.03380409\n",
      "Iteration 751, loss = 0.03368513\n",
      "Iteration 752, loss = 0.03358091\n",
      "Iteration 753, loss = 0.03345041\n",
      "Iteration 754, loss = 0.03334667\n",
      "Iteration 755, loss = 0.03328240\n",
      "Iteration 756, loss = 0.03313633\n",
      "Iteration 757, loss = 0.03303536\n",
      "Iteration 758, loss = 0.03294501\n",
      "Iteration 759, loss = 0.03285906\n",
      "Iteration 760, loss = 0.03274784\n",
      "Iteration 761, loss = 0.03262259\n",
      "Iteration 762, loss = 0.03251300\n",
      "Iteration 763, loss = 0.03240589\n",
      "Iteration 764, loss = 0.03231380\n",
      "Iteration 765, loss = 0.03222295\n",
      "Iteration 766, loss = 0.03209515\n",
      "Iteration 767, loss = 0.03200036\n",
      "Iteration 768, loss = 0.03189693\n",
      "Iteration 769, loss = 0.03184966\n",
      "Iteration 770, loss = 0.03171482\n",
      "Iteration 771, loss = 0.03160133\n",
      "Iteration 772, loss = 0.03151929\n",
      "Iteration 773, loss = 0.03138810\n",
      "Iteration 774, loss = 0.03129353\n",
      "Iteration 775, loss = 0.03120187\n",
      "Iteration 776, loss = 0.03113212\n",
      "Iteration 777, loss = 0.03098942\n",
      "Iteration 778, loss = 0.03090700\n",
      "Iteration 779, loss = 0.03082196\n",
      "Iteration 780, loss = 0.03073463\n",
      "Iteration 781, loss = 0.03061149\n",
      "Iteration 782, loss = 0.03054079\n",
      "Iteration 783, loss = 0.03045625\n",
      "Iteration 784, loss = 0.03033421\n",
      "Iteration 785, loss = 0.03023845\n",
      "Iteration 786, loss = 0.03013071\n",
      "Iteration 787, loss = 0.03007419\n",
      "Iteration 788, loss = 0.02996226\n",
      "Iteration 789, loss = 0.02987928\n",
      "Iteration 790, loss = 0.02974996\n",
      "Iteration 791, loss = 0.02967936\n",
      "Iteration 792, loss = 0.02957526\n",
      "Iteration 793, loss = 0.02949869\n",
      "Iteration 794, loss = 0.02939110\n",
      "Iteration 795, loss = 0.02929476\n",
      "Iteration 796, loss = 0.02920065\n",
      "Iteration 797, loss = 0.02913376\n",
      "Iteration 798, loss = 0.02902025\n",
      "Iteration 799, loss = 0.02893980\n",
      "Iteration 800, loss = 0.02884894\n",
      "Iteration 801, loss = 0.02880701\n",
      "Iteration 802, loss = 0.02869424\n",
      "Iteration 803, loss = 0.02858148\n",
      "Iteration 804, loss = 0.02850684\n",
      "Iteration 805, loss = 0.02841583\n",
      "Iteration 806, loss = 0.02830884\n",
      "Iteration 807, loss = 0.02825141\n",
      "Iteration 808, loss = 0.02814650\n",
      "Iteration 809, loss = 0.02808607\n",
      "Iteration 810, loss = 0.02795382\n",
      "Iteration 811, loss = 0.02789715\n",
      "Iteration 812, loss = 0.02776658\n",
      "Iteration 813, loss = 0.02774467\n",
      "Iteration 814, loss = 0.02762885\n",
      "Iteration 815, loss = 0.02751094\n",
      "Iteration 816, loss = 0.02743468\n",
      "Iteration 817, loss = 0.02734964\n",
      "Iteration 818, loss = 0.02726128\n",
      "Iteration 819, loss = 0.02720554\n",
      "Iteration 820, loss = 0.02712161\n",
      "Iteration 821, loss = 0.02702177\n",
      "Iteration 822, loss = 0.02692900\n",
      "Iteration 823, loss = 0.02685070\n",
      "Iteration 824, loss = 0.02675923\n",
      "Iteration 825, loss = 0.02666494\n",
      "Iteration 826, loss = 0.02660528\n",
      "Iteration 827, loss = 0.02653013\n",
      "Iteration 828, loss = 0.02642921\n",
      "Iteration 829, loss = 0.02638626\n",
      "Iteration 830, loss = 0.02627030\n",
      "Iteration 831, loss = 0.02619768\n",
      "Iteration 832, loss = 0.02612759\n",
      "Iteration 833, loss = 0.02604512\n",
      "Iteration 834, loss = 0.02598760\n",
      "Iteration 835, loss = 0.02591274\n",
      "Iteration 836, loss = 0.02579180\n",
      "Iteration 837, loss = 0.02572128\n",
      "Iteration 838, loss = 0.02564537\n",
      "Iteration 839, loss = 0.02554597\n",
      "Iteration 840, loss = 0.02548856\n",
      "Iteration 841, loss = 0.02539143\n",
      "Iteration 842, loss = 0.02532010\n",
      "Iteration 843, loss = 0.02525388\n",
      "Iteration 844, loss = 0.02516267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 845, loss = 0.02508487\n",
      "Iteration 846, loss = 0.02502975\n",
      "Iteration 847, loss = 0.02492677\n",
      "Iteration 848, loss = 0.02487439\n",
      "Iteration 849, loss = 0.02477220\n",
      "Iteration 850, loss = 0.02470341\n",
      "Iteration 851, loss = 0.02462522\n",
      "Iteration 852, loss = 0.02455498\n",
      "Iteration 853, loss = 0.02448660\n",
      "Iteration 854, loss = 0.02440521\n",
      "Iteration 855, loss = 0.02433758\n",
      "Iteration 856, loss = 0.02424862\n",
      "Iteration 857, loss = 0.02418699\n",
      "Iteration 858, loss = 0.02410087\n",
      "Iteration 859, loss = 0.02403383\n",
      "Iteration 860, loss = 0.02396144\n",
      "Iteration 861, loss = 0.02389049\n",
      "Iteration 862, loss = 0.02379865\n",
      "Iteration 863, loss = 0.02373068\n",
      "Iteration 864, loss = 0.02363994\n",
      "Iteration 865, loss = 0.02356951\n",
      "Iteration 866, loss = 0.02351022\n",
      "Iteration 867, loss = 0.02341964\n",
      "Iteration 868, loss = 0.02334378\n",
      "Iteration 869, loss = 0.02328385\n",
      "Iteration 870, loss = 0.02320987\n",
      "Iteration 871, loss = 0.02314301\n",
      "Iteration 872, loss = 0.02308524\n",
      "Iteration 873, loss = 0.02300496\n",
      "Iteration 874, loss = 0.02293436\n",
      "Iteration 875, loss = 0.02285461\n",
      "Iteration 876, loss = 0.02278161\n",
      "Iteration 877, loss = 0.02270662\n",
      "Iteration 878, loss = 0.02265202\n",
      "Iteration 879, loss = 0.02258910\n",
      "Iteration 880, loss = 0.02252017\n",
      "Iteration 881, loss = 0.02244314\n",
      "Iteration 882, loss = 0.02237236\n",
      "Iteration 883, loss = 0.02231735\n",
      "Iteration 884, loss = 0.02226095\n",
      "Iteration 885, loss = 0.02216785\n",
      "Iteration 886, loss = 0.02214353\n",
      "Iteration 887, loss = 0.02202677\n",
      "Iteration 888, loss = 0.02196411\n",
      "Iteration 889, loss = 0.02188069\n",
      "Iteration 890, loss = 0.02184332\n",
      "Iteration 891, loss = 0.02176584\n",
      "Iteration 892, loss = 0.02169991\n",
      "Iteration 893, loss = 0.02163712\n",
      "Iteration 894, loss = 0.02155953\n",
      "Iteration 895, loss = 0.02149331\n",
      "Iteration 896, loss = 0.02143496\n",
      "Iteration 897, loss = 0.02137479\n",
      "Iteration 898, loss = 0.02131683\n",
      "Iteration 899, loss = 0.02125480\n",
      "Iteration 900, loss = 0.02116673\n",
      "Iteration 901, loss = 0.02111347\n",
      "Iteration 902, loss = 0.02108046\n",
      "Iteration 903, loss = 0.02098447\n",
      "Iteration 904, loss = 0.02092364\n",
      "Iteration 905, loss = 0.02087364\n",
      "Iteration 906, loss = 0.02078725\n",
      "Iteration 907, loss = 0.02074955\n",
      "Iteration 908, loss = 0.02065874\n",
      "Iteration 909, loss = 0.02060038\n",
      "Iteration 910, loss = 0.02053254\n",
      "Iteration 911, loss = 0.02050045\n",
      "Iteration 912, loss = 0.02040968\n",
      "Iteration 913, loss = 0.02034150\n",
      "Iteration 914, loss = 0.02028908\n",
      "Iteration 915, loss = 0.02022954\n",
      "Iteration 916, loss = 0.02015407\n",
      "Iteration 917, loss = 0.02011060\n",
      "Iteration 918, loss = 0.02005044\n",
      "Iteration 919, loss = 0.01998622\n",
      "Iteration 920, loss = 0.01991738\n",
      "Iteration 921, loss = 0.01985881\n",
      "Iteration 922, loss = 0.01978506\n",
      "Iteration 923, loss = 0.01975648\n",
      "Iteration 924, loss = 0.01968679\n",
      "Iteration 925, loss = 0.01961792\n",
      "Iteration 926, loss = 0.01957135\n",
      "Iteration 927, loss = 0.01951816\n",
      "Iteration 928, loss = 0.01945294\n",
      "Iteration 929, loss = 0.01940178\n",
      "Iteration 930, loss = 0.01936541\n",
      "Iteration 931, loss = 0.01928158\n",
      "Iteration 932, loss = 0.01926023\n",
      "Iteration 933, loss = 0.01918329\n",
      "Iteration 934, loss = 0.01912458\n",
      "Iteration 935, loss = 0.01905677\n",
      "Iteration 936, loss = 0.01898072\n",
      "Iteration 937, loss = 0.01892543\n",
      "Iteration 938, loss = 0.01886130\n",
      "Iteration 939, loss = 0.01881141\n",
      "Iteration 940, loss = 0.01878186\n",
      "Iteration 941, loss = 0.01873588\n",
      "Iteration 942, loss = 0.01866640\n",
      "Iteration 943, loss = 0.01859459\n",
      "Iteration 944, loss = 0.01853273\n",
      "Iteration 945, loss = 0.01847815\n",
      "Iteration 946, loss = 0.01841688\n",
      "Iteration 947, loss = 0.01839725\n",
      "Iteration 948, loss = 0.01831406\n",
      "Iteration 949, loss = 0.01825430\n",
      "Iteration 950, loss = 0.01821547\n",
      "Iteration 951, loss = 0.01814306\n",
      "Iteration 952, loss = 0.01808535\n",
      "Iteration 953, loss = 0.01803814\n",
      "Iteration 954, loss = 0.01798248\n",
      "Iteration 955, loss = 0.01793584\n",
      "Iteration 956, loss = 0.01787720\n",
      "Iteration 957, loss = 0.01783118\n",
      "Iteration 958, loss = 0.01777046\n",
      "Iteration 959, loss = 0.01772088\n",
      "Iteration 960, loss = 0.01767126\n",
      "Iteration 961, loss = 0.01760696\n",
      "Iteration 962, loss = 0.01757893\n",
      "Iteration 963, loss = 0.01749357\n",
      "Iteration 964, loss = 0.01744782\n",
      "Iteration 965, loss = 0.01744203\n",
      "Iteration 966, loss = 0.01736517\n",
      "Iteration 967, loss = 0.01731321\n",
      "Iteration 968, loss = 0.01725739\n",
      "Iteration 969, loss = 0.01722911\n",
      "Iteration 970, loss = 0.01715438\n",
      "Iteration 971, loss = 0.01708337\n",
      "Iteration 972, loss = 0.01704807\n",
      "Iteration 973, loss = 0.01699136\n",
      "Iteration 974, loss = 0.01694165\n",
      "Iteration 975, loss = 0.01693428\n",
      "Iteration 976, loss = 0.01683856\n",
      "Iteration 977, loss = 0.01682668\n",
      "Iteration 978, loss = 0.01680430\n",
      "Iteration 979, loss = 0.01671133\n",
      "Iteration 980, loss = 0.01664417\n",
      "Iteration 981, loss = 0.01661717\n",
      "Iteration 982, loss = 0.01653131\n",
      "Iteration 983, loss = 0.01651266\n",
      "Iteration 984, loss = 0.01646183\n",
      "Iteration 985, loss = 0.01640448\n",
      "Iteration 986, loss = 0.01634999\n",
      "Iteration 987, loss = 0.01629896\n",
      "Iteration 988, loss = 0.01625227\n",
      "Iteration 989, loss = 0.01621599\n",
      "Iteration 990, loss = 0.01615712\n",
      "Iteration 991, loss = 0.01611525\n",
      "Iteration 992, loss = 0.01605736\n",
      "Iteration 993, loss = 0.01602554\n",
      "Iteration 994, loss = 0.01598582\n",
      "Iteration 995, loss = 0.01592950\n",
      "Iteration 996, loss = 0.01589738\n",
      "Iteration 997, loss = 0.01586738\n",
      "Iteration 998, loss = 0.01579500\n",
      "Iteration 999, loss = 0.01571794\n",
      "Iteration 1000, loss = 0.01567903\n",
      "Iteration 1001, loss = 0.01561184\n",
      "Iteration 1002, loss = 0.01560308\n",
      "Iteration 1003, loss = 0.01551601\n",
      "Iteration 1004, loss = 0.01547039\n",
      "Iteration 1005, loss = 0.01542220\n",
      "Iteration 1006, loss = 0.01536803\n",
      "Iteration 1007, loss = 0.01533952\n",
      "Iteration 1008, loss = 0.01527427\n",
      "Iteration 1009, loss = 0.01525058\n",
      "Iteration 1010, loss = 0.01517522\n",
      "Iteration 1011, loss = 0.01514060\n",
      "Iteration 1012, loss = 0.01510196\n",
      "Iteration 1013, loss = 0.01504531\n",
      "Iteration 1014, loss = 0.01498518\n",
      "Iteration 1015, loss = 0.01494999\n",
      "Iteration 1016, loss = 0.01492835\n",
      "Iteration 1017, loss = 0.01485998\n",
      "Iteration 1018, loss = 0.01480918\n",
      "Iteration 1019, loss = 0.01474689\n",
      "Iteration 1020, loss = 0.01469670\n",
      "Iteration 1021, loss = 0.01469992\n",
      "Iteration 1022, loss = 0.01460433\n",
      "Iteration 1023, loss = 0.01457667\n",
      "Iteration 1024, loss = 0.01454576\n",
      "Iteration 1025, loss = 0.01450369\n",
      "Iteration 1026, loss = 0.01444091\n",
      "Iteration 1027, loss = 0.01439961\n",
      "Iteration 1028, loss = 0.01435536\n",
      "Iteration 1029, loss = 0.01431846\n",
      "Iteration 1030, loss = 0.01424804\n",
      "Iteration 1031, loss = 0.01419487\n",
      "Iteration 1032, loss = 0.01414445\n",
      "Iteration 1033, loss = 0.01414447\n",
      "Iteration 1034, loss = 0.01406595\n",
      "Iteration 1035, loss = 0.01400916\n",
      "Iteration 1036, loss = 0.01397347\n",
      "Iteration 1037, loss = 0.01394370\n",
      "Iteration 1038, loss = 0.01388544\n",
      "Iteration 1039, loss = 0.01385056\n",
      "Iteration 1040, loss = 0.01380691\n",
      "Iteration 1041, loss = 0.01376602\n",
      "Iteration 1042, loss = 0.01373425\n",
      "Iteration 1043, loss = 0.01367521\n",
      "Iteration 1044, loss = 0.01364811\n",
      "Iteration 1045, loss = 0.01359616\n",
      "Iteration 1046, loss = 0.01356648\n",
      "Iteration 1047, loss = 0.01350402\n",
      "Iteration 1048, loss = 0.01346932\n",
      "Iteration 1049, loss = 0.01345107\n",
      "Iteration 1050, loss = 0.01340843\n",
      "Iteration 1051, loss = 0.01333556\n",
      "Iteration 1052, loss = 0.01329188\n",
      "Iteration 1053, loss = 0.01326272\n",
      "Iteration 1054, loss = 0.01321037\n",
      "Iteration 1055, loss = 0.01317157\n",
      "Iteration 1056, loss = 0.01314021\n",
      "Iteration 1057, loss = 0.01309887\n",
      "Iteration 1058, loss = 0.01304488\n",
      "Iteration 1059, loss = 0.01299861\n",
      "Iteration 1060, loss = 0.01297173\n",
      "Iteration 1061, loss = 0.01292418\n",
      "Iteration 1062, loss = 0.01290695\n",
      "Iteration 1063, loss = 0.01287711\n",
      "Iteration 1064, loss = 0.01284181\n",
      "Iteration 1065, loss = 0.01278604\n",
      "Iteration 1066, loss = 0.01273049\n",
      "Iteration 1067, loss = 0.01270195\n",
      "Iteration 1068, loss = 0.01268230\n",
      "Iteration 1069, loss = 0.01264689\n",
      "Iteration 1070, loss = 0.01258119\n",
      "Iteration 1071, loss = 0.01255259\n",
      "Iteration 1072, loss = 0.01251965\n",
      "Iteration 1073, loss = 0.01247611\n",
      "Iteration 1074, loss = 0.01241435\n",
      "Iteration 1075, loss = 0.01239280\n",
      "Iteration 1076, loss = 0.01234421\n",
      "Iteration 1077, loss = 0.01233827\n",
      "Iteration 1078, loss = 0.01233284\n",
      "Iteration 1079, loss = 0.01223586\n",
      "Iteration 1080, loss = 0.01219989\n",
      "Iteration 1081, loss = 0.01216938\n",
      "Iteration 1082, loss = 0.01213810\n",
      "Iteration 1083, loss = 0.01208601\n",
      "Iteration 1084, loss = 0.01205136\n",
      "Iteration 1085, loss = 0.01201107\n",
      "Iteration 1086, loss = 0.01202319\n",
      "Iteration 1087, loss = 0.01194080\n",
      "Iteration 1088, loss = 0.01192055\n",
      "Iteration 1089, loss = 0.01187984\n",
      "Iteration 1090, loss = 0.01185792\n",
      "Iteration 1091, loss = 0.01179657\n",
      "Iteration 1092, loss = 0.01177518\n",
      "Iteration 1093, loss = 0.01179260\n",
      "Iteration 1094, loss = 0.01171576\n",
      "Iteration 1095, loss = 0.01165737\n",
      "Iteration 1096, loss = 0.01164597\n",
      "Iteration 1097, loss = 0.01158751\n",
      "Iteration 1098, loss = 0.01155370\n",
      "Iteration 1099, loss = 0.01152592\n",
      "Iteration 1100, loss = 0.01148113\n",
      "Iteration 1101, loss = 0.01145060\n",
      "Iteration 1102, loss = 0.01141363\n",
      "Iteration 1103, loss = 0.01139216\n",
      "Iteration 1104, loss = 0.01135292\n",
      "Iteration 1105, loss = 0.01132978\n",
      "Iteration 1106, loss = 0.01128062\n",
      "Iteration 1107, loss = 0.01126559\n",
      "Iteration 1108, loss = 0.01120432\n",
      "Iteration 1109, loss = 0.01117985\n",
      "Iteration 1110, loss = 0.01113464\n",
      "Iteration 1111, loss = 0.01112964\n",
      "Iteration 1112, loss = 0.01109776\n",
      "Iteration 1113, loss = 0.01105270\n",
      "Iteration 1114, loss = 0.01104253\n",
      "Iteration 1115, loss = 0.01099566\n",
      "Iteration 1116, loss = 0.01097025\n",
      "Iteration 1117, loss = 0.01094469\n",
      "Iteration 1118, loss = 0.01090577\n",
      "Iteration 1119, loss = 0.01088415\n",
      "Iteration 1120, loss = 0.01084765\n",
      "Iteration 1121, loss = 0.01080983\n",
      "Iteration 1122, loss = 0.01079977\n",
      "Iteration 1123, loss = 0.01073812\n",
      "Iteration 1124, loss = 0.01073212\n",
      "Iteration 1125, loss = 0.01067504\n",
      "Iteration 1126, loss = 0.01065373\n",
      "Iteration 1127, loss = 0.01063914\n",
      "Iteration 1128, loss = 0.01057766\n",
      "Iteration 1129, loss = 0.01055366\n",
      "Iteration 1130, loss = 0.01054196\n",
      "Iteration 1131, loss = 0.01049997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1132, loss = 0.01045794\n",
      "Iteration 1133, loss = 0.01044415\n",
      "Iteration 1134, loss = 0.01041729\n",
      "Iteration 1135, loss = 0.01040878\n",
      "Iteration 1136, loss = 0.01035312\n",
      "Iteration 1137, loss = 0.01032180\n",
      "Iteration 1138, loss = 0.01031520\n",
      "Iteration 1139, loss = 0.01026797\n",
      "Iteration 1140, loss = 0.01023756\n",
      "Iteration 1141, loss = 0.01021447\n",
      "Iteration 1142, loss = 0.01016579\n",
      "Iteration 1143, loss = 0.01014378\n",
      "Iteration 1144, loss = 0.01011967\n",
      "Iteration 1145, loss = 0.01008453\n",
      "Iteration 1146, loss = 0.01008510\n",
      "Iteration 1147, loss = 0.01003967\n",
      "Iteration 1148, loss = 0.00999913\n",
      "Iteration 1149, loss = 0.00998000\n",
      "Iteration 1150, loss = 0.00993335\n",
      "Iteration 1151, loss = 0.00992592\n",
      "Iteration 1152, loss = 0.00987819\n",
      "Iteration 1153, loss = 0.00988865\n",
      "Iteration 1154, loss = 0.00984164\n",
      "Iteration 1155, loss = 0.00982514\n",
      "Iteration 1156, loss = 0.00979711\n",
      "Iteration 1157, loss = 0.00974795\n",
      "Iteration 1158, loss = 0.00973351\n",
      "Iteration 1159, loss = 0.00971647\n",
      "Iteration 1160, loss = 0.00968529\n",
      "Iteration 1161, loss = 0.00966235\n",
      "Iteration 1162, loss = 0.00961563\n",
      "Iteration 1163, loss = 0.00959903\n",
      "Iteration 1164, loss = 0.00956752\n",
      "Iteration 1165, loss = 0.00956030\n",
      "Iteration 1166, loss = 0.00952404\n",
      "Iteration 1167, loss = 0.00948813\n",
      "Iteration 1168, loss = 0.00945087\n",
      "Iteration 1169, loss = 0.00943666\n",
      "Iteration 1170, loss = 0.00942477\n",
      "Iteration 1171, loss = 0.00938533\n",
      "Iteration 1172, loss = 0.00936075\n",
      "Iteration 1173, loss = 0.00934004\n",
      "Iteration 1174, loss = 0.00930864\n",
      "Iteration 1175, loss = 0.00928962\n",
      "Iteration 1176, loss = 0.00925738\n",
      "Iteration 1177, loss = 0.00922850\n",
      "Iteration 1178, loss = 0.00920649\n",
      "Iteration 1179, loss = 0.00920918\n",
      "Iteration 1180, loss = 0.00916040\n",
      "Iteration 1181, loss = 0.00912582\n",
      "Iteration 1182, loss = 0.00911054\n",
      "Iteration 1183, loss = 0.00907589\n",
      "Iteration 1184, loss = 0.00905202\n",
      "Iteration 1185, loss = 0.00901139\n",
      "Iteration 1186, loss = 0.00900965\n",
      "Iteration 1187, loss = 0.00902748\n",
      "Iteration 1188, loss = 0.00896305\n",
      "Iteration 1189, loss = 0.00893508\n",
      "Iteration 1190, loss = 0.00890339\n",
      "Iteration 1191, loss = 0.00890421\n",
      "Iteration 1192, loss = 0.00883958\n",
      "Iteration 1193, loss = 0.00882969\n",
      "Iteration 1194, loss = 0.00882461\n",
      "Iteration 1195, loss = 0.00880407\n",
      "Iteration 1196, loss = 0.00880173\n",
      "Iteration 1197, loss = 0.00877102\n",
      "Iteration 1198, loss = 0.00872737\n",
      "Iteration 1199, loss = 0.00868065\n",
      "Iteration 1200, loss = 0.00866399\n",
      "Iteration 1201, loss = 0.00864908\n",
      "Iteration 1202, loss = 0.00862835\n",
      "Iteration 1203, loss = 0.00863914\n",
      "Iteration 1204, loss = 0.00859488\n",
      "Iteration 1205, loss = 0.00855754\n",
      "Iteration 1206, loss = 0.00852227\n",
      "Iteration 1207, loss = 0.00849884\n",
      "Iteration 1208, loss = 0.00847706\n",
      "Iteration 1209, loss = 0.00848579\n",
      "Iteration 1210, loss = 0.00842616\n",
      "Iteration 1211, loss = 0.00841098\n",
      "Iteration 1212, loss = 0.00841626\n",
      "Iteration 1213, loss = 0.00838238\n",
      "Iteration 1214, loss = 0.00834975\n",
      "Iteration 1215, loss = 0.00834057\n",
      "Iteration 1216, loss = 0.00829810\n",
      "Iteration 1217, loss = 0.00827590\n",
      "Iteration 1218, loss = 0.00825851\n",
      "Iteration 1219, loss = 0.00827940\n",
      "Iteration 1220, loss = 0.00821964\n",
      "Iteration 1221, loss = 0.00819749\n",
      "Iteration 1222, loss = 0.00816450\n",
      "Iteration 1223, loss = 0.00815909\n",
      "Iteration 1224, loss = 0.00811759\n",
      "Iteration 1225, loss = 0.00811838\n",
      "Iteration 1226, loss = 0.00809047\n",
      "Iteration 1227, loss = 0.00805854\n",
      "Iteration 1228, loss = 0.00802837\n",
      "Iteration 1229, loss = 0.00803448\n",
      "Iteration 1230, loss = 0.00800184\n",
      "Iteration 1231, loss = 0.00798833\n",
      "Iteration 1232, loss = 0.00794651\n",
      "Iteration 1233, loss = 0.00791101\n",
      "Iteration 1234, loss = 0.00794509\n",
      "Iteration 1235, loss = 0.00791792\n",
      "Iteration 1236, loss = 0.00787402\n",
      "Iteration 1237, loss = 0.00785415\n",
      "Iteration 1238, loss = 0.00782778\n",
      "Iteration 1239, loss = 0.00783179\n",
      "Iteration 1240, loss = 0.00777789\n",
      "Iteration 1241, loss = 0.00776124\n",
      "Iteration 1242, loss = 0.00773522\n",
      "Iteration 1243, loss = 0.00771786\n",
      "Iteration 1244, loss = 0.00771524\n",
      "Iteration 1245, loss = 0.00768473\n",
      "Iteration 1246, loss = 0.00767988\n",
      "Iteration 1247, loss = 0.00765002\n",
      "Iteration 1248, loss = 0.00761618\n",
      "Iteration 1249, loss = 0.00763324\n",
      "Iteration 1250, loss = 0.00760840\n",
      "Iteration 1251, loss = 0.00755624\n",
      "Iteration 1252, loss = 0.00754609\n",
      "Iteration 1253, loss = 0.00750676\n",
      "Iteration 1254, loss = 0.00751157\n",
      "Iteration 1255, loss = 0.00752276\n",
      "Iteration 1256, loss = 0.00746722\n",
      "Iteration 1257, loss = 0.00743645\n",
      "Iteration 1258, loss = 0.00741154\n",
      "Iteration 1259, loss = 0.00741910\n",
      "Iteration 1260, loss = 0.00738670\n",
      "Iteration 1261, loss = 0.00738355\n",
      "Iteration 1262, loss = 0.00733854\n",
      "Iteration 1263, loss = 0.00731395\n",
      "Iteration 1264, loss = 0.00731066\n",
      "Iteration 1265, loss = 0.00731820\n",
      "Iteration 1266, loss = 0.00727562\n",
      "Iteration 1267, loss = 0.00724685\n",
      "Iteration 1268, loss = 0.00724734\n",
      "Iteration 1269, loss = 0.00719765\n",
      "Iteration 1270, loss = 0.00717529\n",
      "Iteration 1271, loss = 0.00717454\n",
      "Iteration 1272, loss = 0.00717862\n",
      "Iteration 1273, loss = 0.00715065\n",
      "Iteration 1274, loss = 0.00710977\n",
      "Iteration 1275, loss = 0.00711870\n",
      "Iteration 1276, loss = 0.00709185\n",
      "Iteration 1277, loss = 0.00706724\n",
      "Iteration 1278, loss = 0.00707694\n",
      "Iteration 1279, loss = 0.00703408\n",
      "Iteration 1280, loss = 0.00700252\n",
      "Iteration 1281, loss = 0.00699277\n",
      "Iteration 1282, loss = 0.00697786\n",
      "Iteration 1283, loss = 0.00695585\n",
      "Iteration 1284, loss = 0.00693898\n",
      "Iteration 1285, loss = 0.00690471\n",
      "Iteration 1286, loss = 0.00691077\n",
      "Iteration 1287, loss = 0.00688538\n",
      "Iteration 1288, loss = 0.00685720\n",
      "Iteration 1289, loss = 0.00687205\n",
      "Iteration 1290, loss = 0.00681542\n",
      "Iteration 1291, loss = 0.00682314\n",
      "Iteration 1292, loss = 0.00680054\n",
      "Iteration 1293, loss = 0.00681308\n",
      "Iteration 1294, loss = 0.00680103\n",
      "Iteration 1295, loss = 0.00675719\n",
      "Iteration 1296, loss = 0.00673955\n",
      "Iteration 1297, loss = 0.00671139\n",
      "Iteration 1298, loss = 0.00669932\n",
      "Iteration 1299, loss = 0.00670299\n",
      "Iteration 1300, loss = 0.00667678\n",
      "Iteration 1301, loss = 0.00663199\n",
      "Iteration 1302, loss = 0.00662996\n",
      "Iteration 1303, loss = 0.00661145\n",
      "Iteration 1304, loss = 0.00659175\n",
      "Iteration 1305, loss = 0.00656760\n",
      "Iteration 1306, loss = 0.00656499\n",
      "Iteration 1307, loss = 0.00656587\n",
      "Iteration 1308, loss = 0.00651963\n",
      "Iteration 1309, loss = 0.00655162\n",
      "Iteration 1310, loss = 0.00649647\n",
      "Iteration 1311, loss = 0.00646812\n",
      "Iteration 1312, loss = 0.00645788\n",
      "Iteration 1313, loss = 0.00644142\n",
      "Iteration 1314, loss = 0.00643366\n",
      "Iteration 1315, loss = 0.00640490\n",
      "Iteration 1316, loss = 0.00639540\n",
      "Iteration 1317, loss = 0.00636950\n",
      "Iteration 1318, loss = 0.00636282\n",
      "Iteration 1319, loss = 0.00633904\n",
      "Iteration 1320, loss = 0.00634024\n",
      "Iteration 1321, loss = 0.00629867\n",
      "Iteration 1322, loss = 0.00633214\n",
      "Iteration 1323, loss = 0.00629455\n",
      "Iteration 1324, loss = 0.00626784\n",
      "Iteration 1325, loss = 0.00627708\n",
      "Iteration 1326, loss = 0.00621882\n",
      "Iteration 1327, loss = 0.00622111\n",
      "Iteration 1328, loss = 0.00620060\n",
      "Iteration 1329, loss = 0.00618428\n",
      "Iteration 1330, loss = 0.00616958\n",
      "Iteration 1331, loss = 0.00614917\n",
      "Iteration 1332, loss = 0.00614630\n",
      "Iteration 1333, loss = 0.00612744\n",
      "Iteration 1334, loss = 0.00610265\n",
      "Iteration 1335, loss = 0.00611670\n",
      "Iteration 1336, loss = 0.00607081\n",
      "Iteration 1337, loss = 0.00606307\n",
      "Iteration 1338, loss = 0.00604926\n",
      "Iteration 1339, loss = 0.00601811\n",
      "Iteration 1340, loss = 0.00603059\n",
      "Iteration 1341, loss = 0.00600571\n",
      "Iteration 1342, loss = 0.00598819\n",
      "Iteration 1343, loss = 0.00599220\n",
      "Iteration 1344, loss = 0.00599689\n",
      "Iteration 1345, loss = 0.00595275\n",
      "Iteration 1346, loss = 0.00592952\n",
      "Iteration 1347, loss = 0.00592729\n",
      "Iteration 1348, loss = 0.00592297\n",
      "Iteration 1349, loss = 0.00588893\n",
      "Iteration 1350, loss = 0.00588203\n",
      "Iteration 1351, loss = 0.00586705\n",
      "Iteration 1352, loss = 0.00583641\n",
      "Iteration 1353, loss = 0.00585905\n",
      "Iteration 1354, loss = 0.00582904\n",
      "Iteration 1355, loss = 0.00580471\n",
      "Iteration 1356, loss = 0.00578996\n",
      "Iteration 1357, loss = 0.00577492\n",
      "Iteration 1358, loss = 0.00576244\n",
      "Iteration 1359, loss = 0.00573274\n",
      "Iteration 1360, loss = 0.00573037\n",
      "Iteration 1361, loss = 0.00571138\n",
      "Iteration 1362, loss = 0.00571079\n",
      "Iteration 1363, loss = 0.00567821\n",
      "Iteration 1364, loss = 0.00569107\n",
      "Iteration 1365, loss = 0.00566277\n",
      "Iteration 1366, loss = 0.00564549\n",
      "Iteration 1367, loss = 0.00565931\n",
      "Iteration 1368, loss = 0.00562975\n",
      "Iteration 1369, loss = 0.00562815\n",
      "Iteration 1370, loss = 0.00561216\n",
      "Iteration 1371, loss = 0.00558734\n",
      "Iteration 1372, loss = 0.00556399\n",
      "Iteration 1373, loss = 0.00557804\n",
      "Iteration 1374, loss = 0.00555494\n",
      "Iteration 1375, loss = 0.00553872\n",
      "Iteration 1376, loss = 0.00550450\n",
      "Iteration 1377, loss = 0.00550707\n",
      "Iteration 1378, loss = 0.00551203\n",
      "Iteration 1379, loss = 0.00547955\n",
      "Iteration 1380, loss = 0.00547535\n",
      "Iteration 1381, loss = 0.00545542\n",
      "Iteration 1382, loss = 0.00543977\n",
      "Iteration 1383, loss = 0.00543551\n",
      "Iteration 1384, loss = 0.00541373\n",
      "Iteration 1385, loss = 0.00538333\n",
      "Iteration 1386, loss = 0.00537824\n",
      "Iteration 1387, loss = 0.00537620\n",
      "Iteration 1388, loss = 0.00538872\n",
      "Iteration 1389, loss = 0.00535746\n",
      "Iteration 1390, loss = 0.00534204\n",
      "Iteration 1391, loss = 0.00534415\n",
      "Iteration 1392, loss = 0.00531512\n",
      "Iteration 1393, loss = 0.00530801\n",
      "Iteration 1394, loss = 0.00530380\n",
      "Iteration 1395, loss = 0.00527435\n",
      "Iteration 1396, loss = 0.00525637\n",
      "Iteration 1397, loss = 0.00525280\n",
      "Iteration 1398, loss = 0.00525097\n",
      "Iteration 1399, loss = 0.00524213\n",
      "Iteration 1400, loss = 0.00522299\n",
      "Iteration 1401, loss = 0.00521260\n",
      "Iteration 1402, loss = 0.00520213\n",
      "Iteration 1403, loss = 0.00516579\n",
      "Iteration 1404, loss = 0.00517415\n",
      "Iteration 1405, loss = 0.00516485\n",
      "Iteration 1406, loss = 0.00514073\n",
      "Iteration 1407, loss = 0.00512831\n",
      "Iteration 1408, loss = 0.00515128\n",
      "Iteration 1409, loss = 0.00513357\n",
      "Iteration 1410, loss = 0.00512697\n",
      "Iteration 1411, loss = 0.00512570\n",
      "Iteration 1412, loss = 0.00506688\n",
      "Iteration 1413, loss = 0.00508081\n",
      "Iteration 1414, loss = 0.00510072\n",
      "Iteration 1415, loss = 0.00503238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1416, loss = 0.00501576\n",
      "Iteration 1417, loss = 0.00502820\n",
      "Iteration 1418, loss = 0.00503110\n",
      "Iteration 1419, loss = 0.00499660\n",
      "Iteration 1420, loss = 0.00496800\n",
      "Iteration 1421, loss = 0.00497911\n",
      "Iteration 1422, loss = 0.00497684\n",
      "Iteration 1423, loss = 0.00495562\n",
      "Iteration 1424, loss = 0.00492201\n",
      "Iteration 1425, loss = 0.00491321\n",
      "Iteration 1426, loss = 0.00491828\n",
      "Iteration 1427, loss = 0.00490080\n",
      "Iteration 1428, loss = 0.00489168\n",
      "Iteration 1429, loss = 0.00488527\n",
      "Iteration 1430, loss = 0.00487456\n",
      "Iteration 1431, loss = 0.00484325\n",
      "Iteration 1432, loss = 0.00485456\n",
      "Iteration 1433, loss = 0.00483306\n",
      "Iteration 1434, loss = 0.00482092\n",
      "Iteration 1435, loss = 0.00482895\n",
      "Iteration 1436, loss = 0.00479661\n",
      "Iteration 1437, loss = 0.00481064\n",
      "Iteration 1438, loss = 0.00478922\n",
      "Iteration 1439, loss = 0.00477126\n",
      "Iteration 1440, loss = 0.00476209\n",
      "Iteration 1441, loss = 0.00474823\n",
      "Iteration 1442, loss = 0.00473306\n",
      "Iteration 1443, loss = 0.00473048\n",
      "Iteration 1444, loss = 0.00472175\n",
      "Iteration 1445, loss = 0.00471446\n",
      "Iteration 1446, loss = 0.00468977\n",
      "Iteration 1447, loss = 0.00469667\n",
      "Iteration 1448, loss = 0.00466485\n",
      "Iteration 1449, loss = 0.00464866\n",
      "Iteration 1450, loss = 0.00465353\n",
      "Iteration 1451, loss = 0.00464993\n",
      "Iteration 1452, loss = 0.00463707\n",
      "Iteration 1453, loss = 0.00461230\n",
      "Iteration 1454, loss = 0.00463238\n",
      "Iteration 1455, loss = 0.00459608\n",
      "Iteration 1456, loss = 0.00458915\n",
      "Iteration 1457, loss = 0.00461139\n",
      "Iteration 1458, loss = 0.00458639\n",
      "Iteration 1459, loss = 0.00455935\n",
      "Iteration 1460, loss = 0.00455175\n",
      "Iteration 1461, loss = 0.00453756\n",
      "Iteration 1462, loss = 0.00454406\n",
      "Iteration 1463, loss = 0.00452168\n",
      "Iteration 1464, loss = 0.00451762\n",
      "Iteration 1465, loss = 0.00451109\n",
      "Iteration 1466, loss = 0.00450397\n",
      "Iteration 1467, loss = 0.00447771\n",
      "Iteration 1468, loss = 0.00446302\n",
      "Iteration 1469, loss = 0.00447725\n",
      "Iteration 1470, loss = 0.00448069\n",
      "Iteration 1471, loss = 0.00442995\n",
      "Iteration 1472, loss = 0.00444202\n",
      "Iteration 1473, loss = 0.00441362\n",
      "Iteration 1474, loss = 0.00441787\n",
      "Iteration 1475, loss = 0.00439338\n",
      "Iteration 1476, loss = 0.00440741\n",
      "Iteration 1477, loss = 0.00436574\n",
      "Iteration 1478, loss = 0.00436717\n",
      "Iteration 1479, loss = 0.00437161\n",
      "Iteration 1480, loss = 0.00436017\n",
      "Iteration 1481, loss = 0.00435007\n",
      "Iteration 1482, loss = 0.00433766\n",
      "Iteration 1483, loss = 0.00430814\n",
      "Iteration 1484, loss = 0.00432813\n",
      "Iteration 1485, loss = 0.00431824\n",
      "Iteration 1486, loss = 0.00429614\n",
      "Iteration 1487, loss = 0.00429236\n",
      "Iteration 1488, loss = 0.00429788\n",
      "Iteration 1489, loss = 0.00430860\n",
      "Iteration 1490, loss = 0.00427071\n",
      "Iteration 1491, loss = 0.00424019\n",
      "Iteration 1492, loss = 0.00423853\n",
      "Iteration 1493, loss = 0.00423703\n",
      "Iteration 1494, loss = 0.00421980\n",
      "Iteration 1495, loss = 0.00421123\n",
      "Iteration 1496, loss = 0.00420020\n",
      "Iteration 1497, loss = 0.00418169\n",
      "Iteration 1498, loss = 0.00420201\n",
      "Iteration 1499, loss = 0.00419016\n",
      "Iteration 1500, loss = 0.00417672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UsuÃ¡rio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver = 'adam', activation = 'relu',\n",
    "                                   hidden_layer_sizes = (2, 2)\n",
    "                                  )\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c935c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472cf825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c26880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd3c1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4db8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb2b8",
   "metadata": {},
   "source": [
    "## Base census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a062626",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census.pkl', 'rb') as f:\n",
    "    X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34da5eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_treinamento.shape, y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43aeac82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "232e8e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41866414\n",
      "Iteration 2, loss = 0.32837319\n",
      "Iteration 3, loss = 0.31624729\n",
      "Iteration 4, loss = 0.30800259\n",
      "Iteration 5, loss = 0.30283971\n",
      "Iteration 6, loss = 0.29840483\n",
      "Iteration 7, loss = 0.29461851\n",
      "Iteration 8, loss = 0.29223217\n",
      "Iteration 9, loss = 0.29000989\n",
      "Iteration 10, loss = 0.28767775\n",
      "Iteration 11, loss = 0.28575210\n",
      "Iteration 12, loss = 0.28401017\n",
      "Iteration 13, loss = 0.28306060\n",
      "Iteration 14, loss = 0.28057592\n",
      "Iteration 15, loss = 0.27847853\n",
      "Iteration 16, loss = 0.27678169\n",
      "Iteration 17, loss = 0.27513673\n",
      "Iteration 18, loss = 0.27415479\n",
      "Iteration 19, loss = 0.27231049\n",
      "Iteration 20, loss = 0.27124734\n",
      "Iteration 21, loss = 0.26982637\n",
      "Iteration 22, loss = 0.26776403\n",
      "Iteration 23, loss = 0.26575616\n",
      "Iteration 24, loss = 0.26641113\n",
      "Iteration 25, loss = 0.26343342\n",
      "Iteration 26, loss = 0.26257854\n",
      "Iteration 27, loss = 0.26056338\n",
      "Iteration 28, loss = 0.25938281\n",
      "Iteration 29, loss = 0.25764134\n",
      "Iteration 30, loss = 0.25629481\n",
      "Iteration 31, loss = 0.25562434\n",
      "Iteration 32, loss = 0.25415945\n",
      "Iteration 33, loss = 0.25282662\n",
      "Iteration 34, loss = 0.25114189\n",
      "Iteration 35, loss = 0.25071030\n",
      "Iteration 36, loss = 0.24777673\n",
      "Iteration 37, loss = 0.24713574\n",
      "Iteration 38, loss = 0.24680206\n",
      "Iteration 39, loss = 0.24523571\n",
      "Iteration 40, loss = 0.24492859\n",
      "Iteration 41, loss = 0.24241769\n",
      "Iteration 42, loss = 0.24156149\n",
      "Iteration 43, loss = 0.24060255\n",
      "Iteration 44, loss = 0.23965704\n",
      "Iteration 45, loss = 0.23964120\n",
      "Iteration 46, loss = 0.23717004\n",
      "Iteration 47, loss = 0.23636894\n",
      "Iteration 48, loss = 0.23559818\n",
      "Iteration 49, loss = 0.23456657\n",
      "Iteration 50, loss = 0.23379075\n",
      "Iteration 51, loss = 0.23325837\n",
      "Iteration 52, loss = 0.23273607\n",
      "Iteration 53, loss = 0.23003746\n",
      "Iteration 54, loss = 0.23048502\n",
      "Iteration 55, loss = 0.22894394\n",
      "Iteration 56, loss = 0.22709041\n",
      "Iteration 57, loss = 0.22666085\n",
      "Iteration 58, loss = 0.22667675\n",
      "Iteration 59, loss = 0.22535969\n",
      "Iteration 60, loss = 0.22462396\n",
      "Iteration 61, loss = 0.22402007\n",
      "Iteration 62, loss = 0.22253866\n",
      "Iteration 63, loss = 0.22148578\n",
      "Iteration 64, loss = 0.22107451\n",
      "Iteration 65, loss = 0.22019516\n",
      "Iteration 66, loss = 0.21881803\n",
      "Iteration 67, loss = 0.21886662\n",
      "Iteration 68, loss = 0.21793479\n",
      "Iteration 69, loss = 0.21691021\n",
      "Iteration 70, loss = 0.21593718\n",
      "Iteration 71, loss = 0.21518343\n",
      "Iteration 72, loss = 0.21395018\n",
      "Iteration 73, loss = 0.21408568\n",
      "Iteration 74, loss = 0.21359763\n",
      "Iteration 75, loss = 0.21398472\n",
      "Iteration 76, loss = 0.21203364\n",
      "Iteration 77, loss = 0.21138283\n",
      "Iteration 78, loss = 0.21026675\n",
      "Iteration 79, loss = 0.20959929\n",
      "Iteration 80, loss = 0.21068075\n",
      "Iteration 81, loss = 0.20923793\n",
      "Iteration 82, loss = 0.20846988\n",
      "Iteration 83, loss = 0.20662475\n",
      "Iteration 84, loss = 0.20630904\n",
      "Iteration 85, loss = 0.20559029\n",
      "Iteration 86, loss = 0.20566636\n",
      "Iteration 87, loss = 0.20450740\n",
      "Iteration 88, loss = 0.20540516\n",
      "Iteration 89, loss = 0.20458907\n",
      "Iteration 90, loss = 0.20302056\n",
      "Iteration 91, loss = 0.20320586\n",
      "Iteration 92, loss = 0.20207453\n",
      "Iteration 93, loss = 0.20128901\n",
      "Iteration 94, loss = 0.20037743\n",
      "Iteration 95, loss = 0.20046865\n",
      "Iteration 96, loss = 0.19965743\n",
      "Iteration 97, loss = 0.19924207\n",
      "Iteration 98, loss = 0.19826094\n",
      "Iteration 99, loss = 0.19834960\n",
      "Iteration 100, loss = 0.19651069\n",
      "Iteration 101, loss = 0.19688712\n",
      "Iteration 102, loss = 0.19645357\n",
      "Iteration 103, loss = 0.19599757\n",
      "Iteration 104, loss = 0.19606627\n",
      "Iteration 105, loss = 0.19481472\n",
      "Iteration 106, loss = 0.19420604\n",
      "Iteration 107, loss = 0.19470948\n",
      "Iteration 108, loss = 0.19453612\n",
      "Iteration 109, loss = 0.19308058\n",
      "Iteration 110, loss = 0.19221709\n",
      "Iteration 111, loss = 0.19178237\n",
      "Iteration 112, loss = 0.19178061\n",
      "Iteration 113, loss = 0.19118566\n",
      "Iteration 114, loss = 0.19035894\n",
      "Iteration 115, loss = 0.19022924\n",
      "Iteration 116, loss = 0.18968604\n",
      "Iteration 117, loss = 0.18886186\n",
      "Iteration 118, loss = 0.18951728\n",
      "Iteration 119, loss = 0.18721197\n",
      "Iteration 120, loss = 0.18753744\n",
      "Iteration 121, loss = 0.18768796\n",
      "Iteration 122, loss = 0.18721436\n",
      "Iteration 123, loss = 0.18830540\n",
      "Iteration 124, loss = 0.18875447\n",
      "Iteration 125, loss = 0.18630577\n",
      "Iteration 126, loss = 0.18588283\n",
      "Iteration 127, loss = 0.18564412\n",
      "Iteration 128, loss = 0.18571069\n",
      "Iteration 129, loss = 0.18578634\n",
      "Iteration 130, loss = 0.18331600\n",
      "Iteration 131, loss = 0.18286785\n",
      "Iteration 132, loss = 0.18229857\n",
      "Iteration 133, loss = 0.18229975\n",
      "Iteration 134, loss = 0.18304899\n",
      "Iteration 135, loss = 0.18494747\n",
      "Iteration 136, loss = 0.18101125\n",
      "Iteration 137, loss = 0.18263117\n",
      "Iteration 138, loss = 0.18152307\n",
      "Iteration 139, loss = 0.18058805\n",
      "Iteration 140, loss = 0.18046297\n",
      "Iteration 141, loss = 0.18079854\n",
      "Iteration 142, loss = 0.18036872\n",
      "Iteration 143, loss = 0.17993361\n",
      "Iteration 144, loss = 0.18073254\n",
      "Iteration 145, loss = 0.17787910\n",
      "Iteration 146, loss = 0.17828096\n",
      "Iteration 147, loss = 0.17846581\n",
      "Iteration 148, loss = 0.17812039\n",
      "Iteration 149, loss = 0.17719782\n",
      "Iteration 150, loss = 0.17786356\n",
      "Iteration 151, loss = 0.17701675\n",
      "Iteration 152, loss = 0.17625531\n",
      "Iteration 153, loss = 0.17891381\n",
      "Iteration 154, loss = 0.17593857\n",
      "Iteration 155, loss = 0.17703408\n",
      "Iteration 156, loss = 0.17741380\n",
      "Iteration 157, loss = 0.17564708\n",
      "Iteration 158, loss = 0.17558540\n",
      "Iteration 159, loss = 0.17508996\n",
      "Iteration 160, loss = 0.17451400\n",
      "Iteration 161, loss = 0.17427104\n",
      "Iteration 162, loss = 0.17401282\n",
      "Iteration 163, loss = 0.17320755\n",
      "Iteration 164, loss = 0.17242888\n",
      "Iteration 165, loss = 0.17339410\n",
      "Iteration 166, loss = 0.17430460\n",
      "Iteration 167, loss = 0.17330216\n",
      "Iteration 168, loss = 0.17061410\n",
      "Iteration 169, loss = 0.17144208\n",
      "Iteration 170, loss = 0.17098928\n",
      "Iteration 171, loss = 0.17085588\n",
      "Iteration 172, loss = 0.17008290\n",
      "Iteration 173, loss = 0.16993356\n",
      "Iteration 174, loss = 0.17016377\n",
      "Iteration 175, loss = 0.17036560\n",
      "Iteration 176, loss = 0.17002886\n",
      "Iteration 177, loss = 0.17008614\n",
      "Iteration 178, loss = 0.16896733\n",
      "Iteration 179, loss = 0.17050673\n",
      "Iteration 180, loss = 0.16947315\n",
      "Iteration 181, loss = 0.16816710\n",
      "Iteration 182, loss = 0.16886012\n",
      "Iteration 183, loss = 0.16935433\n",
      "Iteration 184, loss = 0.16785499\n",
      "Iteration 185, loss = 0.16691134\n",
      "Iteration 186, loss = 0.16761601\n",
      "Iteration 187, loss = 0.16587012\n",
      "Iteration 188, loss = 0.16696312\n",
      "Iteration 189, loss = 0.16729807\n",
      "Iteration 190, loss = 0.16666710\n",
      "Iteration 191, loss = 0.16594195\n",
      "Iteration 192, loss = 0.16593597\n",
      "Iteration 193, loss = 0.16638053\n",
      "Iteration 194, loss = 0.16587758\n",
      "Iteration 195, loss = 0.16557988\n",
      "Iteration 196, loss = 0.16406483\n",
      "Iteration 197, loss = 0.16368572\n",
      "Iteration 198, loss = 0.16460591\n",
      "Iteration 199, loss = 0.16371816\n",
      "Iteration 200, loss = 0.16387810\n",
      "Iteration 201, loss = 0.16520657\n",
      "Iteration 202, loss = 0.16500884\n",
      "Iteration 203, loss = 0.16297423\n",
      "Iteration 204, loss = 0.16465286\n",
      "Iteration 205, loss = 0.16374505\n",
      "Iteration 206, loss = 0.16217116\n",
      "Iteration 207, loss = 0.16296591\n",
      "Iteration 208, loss = 0.16290510\n",
      "Iteration 209, loss = 0.16198686\n",
      "Iteration 210, loss = 0.16217622\n",
      "Iteration 211, loss = 0.16289001\n",
      "Iteration 212, loss = 0.16206602\n",
      "Iteration 213, loss = 0.16278352\n",
      "Iteration 214, loss = 0.16270957\n",
      "Iteration 215, loss = 0.16096520\n",
      "Iteration 216, loss = 0.16173805\n",
      "Iteration 217, loss = 0.16209159\n",
      "Iteration 218, loss = 0.16254902\n",
      "Iteration 219, loss = 0.16042861\n",
      "Iteration 220, loss = 0.15940967\n",
      "Iteration 221, loss = 0.16012530\n",
      "Iteration 222, loss = 0.15961859\n",
      "Iteration 223, loss = 0.15879247\n",
      "Iteration 224, loss = 0.15953084\n",
      "Iteration 225, loss = 0.15878454\n",
      "Iteration 226, loss = 0.15973649\n",
      "Iteration 227, loss = 0.15975260\n",
      "Iteration 228, loss = 0.15885781\n",
      "Iteration 229, loss = 0.15900320\n",
      "Iteration 230, loss = 0.15842895\n",
      "Iteration 231, loss = 0.15815897\n",
      "Iteration 232, loss = 0.15858934\n",
      "Iteration 233, loss = 0.15796772\n",
      "Iteration 234, loss = 0.15881396\n",
      "Iteration 235, loss = 0.15639459\n",
      "Iteration 236, loss = 0.15650515\n",
      "Iteration 237, loss = 0.15798705\n",
      "Iteration 238, loss = 0.15652904\n",
      "Iteration 239, loss = 0.15811758\n",
      "Iteration 240, loss = 0.15705628\n",
      "Iteration 241, loss = 0.15631434\n",
      "Iteration 242, loss = 0.15737457\n",
      "Iteration 243, loss = 0.15635868\n",
      "Iteration 244, loss = 0.15544717\n",
      "Iteration 245, loss = 0.15492843\n",
      "Iteration 246, loss = 0.15551371\n",
      "Iteration 247, loss = 0.15440292\n",
      "Iteration 248, loss = 0.15513637\n",
      "Iteration 249, loss = 0.15487979\n",
      "Iteration 250, loss = 0.15350287\n",
      "Iteration 251, loss = 0.15506226\n",
      "Iteration 252, loss = 0.15549198\n",
      "Iteration 253, loss = 0.15410450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.15410725\n",
      "Iteration 255, loss = 0.15326762\n",
      "Iteration 256, loss = 0.15518449\n",
      "Iteration 257, loss = 0.15310618\n",
      "Iteration 258, loss = 0.15528946\n",
      "Iteration 259, loss = 0.15245781\n",
      "Iteration 260, loss = 0.15406605\n",
      "Iteration 261, loss = 0.15376987\n",
      "Iteration 262, loss = 0.15308211\n",
      "Iteration 263, loss = 0.15291433\n",
      "Iteration 264, loss = 0.15238725\n",
      "Iteration 265, loss = 0.15253806\n",
      "Iteration 266, loss = 0.15309094\n",
      "Iteration 267, loss = 0.15144848\n",
      "Iteration 268, loss = 0.15245410\n",
      "Iteration 269, loss = 0.15125327\n",
      "Iteration 270, loss = 0.15129064\n",
      "Iteration 271, loss = 0.15064223\n",
      "Iteration 272, loss = 0.14989850\n",
      "Iteration 273, loss = 0.15116448\n",
      "Iteration 274, loss = 0.15184089\n",
      "Iteration 275, loss = 0.15408926\n",
      "Iteration 276, loss = 0.15102597\n",
      "Iteration 277, loss = 0.15020798\n",
      "Iteration 278, loss = 0.15037010\n",
      "Iteration 279, loss = 0.15033131\n",
      "Iteration 280, loss = 0.14998234\n",
      "Iteration 281, loss = 0.15103814\n",
      "Iteration 282, loss = 0.14958249\n",
      "Iteration 283, loss = 0.15000309\n",
      "Iteration 284, loss = 0.15228962\n",
      "Iteration 285, loss = 0.15041960\n",
      "Iteration 286, loss = 0.15009848\n",
      "Iteration 287, loss = 0.14920234\n",
      "Iteration 288, loss = 0.14967224\n",
      "Iteration 289, loss = 0.14882951\n",
      "Iteration 290, loss = 0.14747499\n",
      "Iteration 291, loss = 0.14872692\n",
      "Iteration 292, loss = 0.14822504\n",
      "Iteration 293, loss = 0.14967687\n",
      "Iteration 294, loss = 0.14730787\n",
      "Iteration 295, loss = 0.14736864\n",
      "Iteration 296, loss = 0.14820209\n",
      "Iteration 297, loss = 0.14828588\n",
      "Iteration 298, loss = 0.14662664\n",
      "Iteration 299, loss = 0.14889857\n",
      "Iteration 300, loss = 0.14679940\n",
      "Iteration 301, loss = 0.14762337\n",
      "Iteration 302, loss = 0.14742695\n",
      "Iteration 303, loss = 0.14546886\n",
      "Iteration 304, loss = 0.14755631\n",
      "Iteration 305, loss = 0.14668296\n",
      "Iteration 306, loss = 0.14626813\n",
      "Iteration 307, loss = 0.14551949\n",
      "Iteration 308, loss = 0.14769966\n",
      "Iteration 309, loss = 0.14561553\n",
      "Iteration 310, loss = 0.14655800\n",
      "Iteration 311, loss = 0.14750627\n",
      "Iteration 312, loss = 0.14584133\n",
      "Iteration 313, loss = 0.14598594\n",
      "Iteration 314, loss = 0.14714909\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(max_iter = 1000, verbose=True, tol=0.000010,\n",
    "                                   hidden_layer_sizes = (55, 55))\n",
    "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac9a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ceaa7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b85599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149437052200614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe1dbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149437052200614"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treinamento, y_census_treinamento)\n",
    "cm.score(X_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b77cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.89      0.88      3693\n",
      "        >50K       0.63      0.57      0.60      1192\n",
      "\n",
      "    accuracy                           0.81      4885\n",
      "   macro avg       0.75      0.73      0.74      4885\n",
      "weighted avg       0.81      0.81      0.81      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
